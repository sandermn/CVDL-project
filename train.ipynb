{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "spread-salad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, sampler\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "\n",
    "from DatasetMedical import DatasetMedical\n",
    "from Unet2D import Unet2D\n",
    "\n",
    "\n",
    "def train(model, train_dl, valid_dl, loss_fn, optimizer, acc_fn, params_path, epochs=1):\n",
    "    start = time.time()\n",
    "    model.cuda()\n",
    "\n",
    "    train_loss, valid_loss = [], []\n",
    "\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train(True)  # Set trainind mode = true\n",
    "                dataloader = train_dl\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "                dataloader = valid_dl\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "\n",
    "            step = 0\n",
    "\n",
    "            # iterate over data\n",
    "            for x, y in dataloader:\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()\n",
    "                step += 1\n",
    "\n",
    "                # forward pass\n",
    "                if phase == 'train':\n",
    "                    # zero the gradients\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(x)\n",
    "                    loss = loss_fn(outputs, y)\n",
    "\n",
    "                    # the backward pass frees the graph memory, so there is no \n",
    "                    # need for torch.no_grad in this training pass\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    # scheduler.step()\n",
    "\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(x)\n",
    "                        loss = loss_fn(outputs, y.long())\n",
    "\n",
    "                # stats - whatever is the phase\n",
    "                acc = acc_fn(outputs, y)\n",
    "\n",
    "                running_acc += acc * dataloader.batch_size\n",
    "                running_loss += loss * dataloader.batch_size\n",
    "\n",
    "                if step % 100 == 0:\n",
    "                    # clear_output(wait=True)\n",
    "                    print('Current step: {}  Loss: {}  Acc: {}  AllocMem (Mb): {}'.format(step, loss, acc,\n",
    "                                                                                          torch.cuda.memory_allocated() / 1024 / 1024))\n",
    "                    # print(torch.cuda.memory_summary())\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloader.dataset)\n",
    "            epoch_acc = running_acc / len(dataloader.dataset)\n",
    "\n",
    "            print('Epoch {}/{}'.format(epoch, epochs - 1))\n",
    "            print('-' * 10)\n",
    "            print('{} Loss: {:.4f} Acc: {}'.format(phase, epoch_loss, epoch_acc))\n",
    "            print('-' * 10)\n",
    "\n",
    "            train_loss.append(epoch_loss) if phase == 'train' else valid_loss.append(epoch_loss)\n",
    "        torch.save(model.state_dict(), params_path + f'{epoch}.pth')\n",
    "    torch.save(model.state_dict(), params_path + 'final.pth')\n",
    "    time_elapsed = time.time() - start\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    return train_loss, valid_loss\n",
    "\n",
    "\n",
    "def acc_metric(predb, yb):\n",
    "    return (predb.argmax(dim=1) == yb.cuda()).float().mean()\n",
    "\n",
    "def dice_metric(predb, yb):\n",
    "    # Ventricle = 2, Background = 0\n",
    "    segmented = 2 * predb.argmax(dim=1)\n",
    "\n",
    "    # TP = 2 - 1 = 1, TN = 0 - 0 = 0, FP = 2 - 0 = 2, FN = 0 - 1 = -1\n",
    "    conf = segmented - yb\n",
    "    TP = (conf == 1).sum()\n",
    "    TN = (conf == 0).sum()\n",
    "    FP = (conf == 2).sum()\n",
    "    FN = (conf == -1).sum()\n",
    "    return 2 * TP / (2 * TP + FP + FN)\n",
    "\n",
    "def batch_to_img(xb, idx):\n",
    "    img = np.array(xb[idx, 0:3])\n",
    "    return img.transpose((1, 2, 0))\n",
    "\n",
    "\n",
    "def predb_to_mask(predb, idx):\n",
    "    p = torch.functional.F.softmax(predb[idx], 0)\n",
    "    return p.argmax(0).cpu()\n",
    "\n",
    "def get_random_folder_split(path):\n",
    "    x_path = path / 'train_gray'\n",
    "    gray_files = os.listdir(x_path)\n",
    "    no_files = len(gray_files)\n",
    "    indices = list(range(no_files))\n",
    "    random.shuffle(indices)\n",
    "    train_files = [gray_files[i] for i in indices[:int(np.floor(0.7*no_files))]]\n",
    "    val_files = [gray_files[i] for i in indices[int(np.floor(0.7*no_files)): int(np.floor(0.85*no_files))]]\n",
    "    test_files = [gray_files[i] for i in indices[int(np.floor(0.85*no_files)):]]\n",
    "    return train_files, val_files, test_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "improving-singing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'train_dl' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4c0206824eca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-4c0206824eca>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# Overskriver transformen her\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;31m#train_dataset.dataset.transform = augmentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mtrain_dl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mvalid_dl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'train_dl' referenced before assignment"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # enable if you want to see some plotting\n",
    "    visual_debug = False\n",
    "    params_path = 'models/model_epoch_'\n",
    "\n",
    "    # batch size\n",
    "    bs = 12\n",
    "\n",
    "    # epochs\n",
    "    epochs_val = 2\n",
    "\n",
    "    # learning rate\n",
    "    learn_rate = 0.01\n",
    "\n",
    "    # sets the matplotlib display backend (most likely not needed)\n",
    "    # mp.use('TkAgg', force=True)\n",
    "    # Preprocessing\n",
    "    # PREPROCESS SKAL GJÃ˜RES HER\n",
    "    mean, std = 0.485, 0.229\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomVerticalFlip(p=0.3),\n",
    "    ])\n",
    "    # Data Augmentation\n",
    "    # LEGG TIL NYE METODER FOR AGUMENTATION HER\n",
    "\n",
    "    # load the training data\n",
    "    # CHANGE \"trans\" TO \"preprocess\" if applying preprocessing (gaussian blur and isotropic pixel size)\n",
    "    base_path = Path('/work/datasets/medical_project/CAMUS_resized')\n",
    "    train_files, val_files, _ = get_random_folder_split(base_path)\n",
    "    train_dataset = DatasetMedical(base_path / 'train_gray', train_files,\n",
    "                                    base_path / 'train_gt', transform_train, gaussian_blur=True)\n",
    "    val_dataset = DatasetMedical(base_path / 'train_gray', val_files,\n",
    "                                    base_path / 'train_gt', transform_train, gaussian_blur=True)\n",
    "    print(len(train_dataset))\n",
    "    # data = DatasetMedical(base_path / 'train_gray',\n",
    "    #                      base_path / 'train_gt', transform=preprocess)\n",
    "    # print(len(data))\n",
    "\n",
    "    # split the training dataset and initialize the data loaders\n",
    "    # train_dataset, valid_dataset, _ = torch.utils.data.random_split(\n",
    "    #     data,\n",
    "    #     (300, 100, 50),\n",
    "    #     generator=torch.Generator().manual_seed(42)\n",
    "    # )\n",
    "    # Overskriver transformen her\n",
    "    #train_dataset.dataset.transform = augmentation\n",
    "    train_dl = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "    valid_dl = DataLoader(valid_dataset, batch_size=bs, shuffle=False)\n",
    "\n",
    "    if visual_debug:\n",
    "        fig, ax = plt.subplots(1, 2)\n",
    "        ax[0].imshow(train_dataset.open_as_array(150))\n",
    "        ax[1].imshow(train_dataset.open_mask(150))\n",
    "        plt.show()\n",
    "\n",
    "    xb, yb = next(iter(train_dl))\n",
    "    print(xb.shape, yb.shape)\n",
    "\n",
    "    # build the Unet2D with one channel as input and 2 channels as output\n",
    "    unet = Unet2D(1, 2)\n",
    "\n",
    "    # loss function and optimizer\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    opt = torch.optim.Adam(unet.parameters(), lr=learn_rate)\n",
    "\n",
    "    # do some training\n",
    "    train_loss, valid_loss = train(unet, train_dl, valid_dl, loss_fn, opt, acc_metric, epochs=epochs_val,\n",
    "                                   params_path=params_path)\n",
    "\n",
    "    # plot training and validation losses\n",
    "    if visual_debug:\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.plot(train_loss, label='Train loss')\n",
    "        plt.plot(valid_loss, label='Valid loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    # predict on the next train batch (is this fair?)\n",
    "    xb, yb = next(iter(train_dl))\n",
    "    with torch.no_grad():\n",
    "        predb = unet(xb.cuda())\n",
    "\n",
    "    # show the predicted segmentations\n",
    "    if visual_debug:\n",
    "        fig, ax = plt.subplots(bs, 3, figsize=(15, bs * 5))\n",
    "        for i in range(bs):\n",
    "            ax[i, 0].imshow(batch_to_img(xb, i))\n",
    "            ax[i, 1].imshow(yb[i])\n",
    "            ax[i, 2].imshow(predb_to_mask(predb, i))\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-variety",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
