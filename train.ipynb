{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "stupid-spare",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torchgeometry as tgm\n",
    "\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, sampler\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "\n",
    "from DatasetMedical import DatasetCAMUS_r, DatasetCAMUS\n",
    "from Unet2D import Unet2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "headed-somalia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dl, valid_dl, loss_fn, optimizer, acc_fn, params_path, epochs=1):\n",
    "    start = time.time()\n",
    "    model.cuda()\n",
    "\n",
    "    train_loss, valid_loss = [], []\n",
    "\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train(True)  # Set trainind mode = true\n",
    "                dataloader = train_dl\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "                dataloader = valid_dl\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "\n",
    "            step = 0\n",
    "\n",
    "            # iterate over data\n",
    "            for x, y in dataloader:\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()\n",
    "                step += 1\n",
    "\n",
    "                # forward pass\n",
    "                if phase == 'train':\n",
    "                    # zero the gradients\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(x)\n",
    "                    \n",
    "                    #print(np.shape(outputs), np.shape(y))\n",
    "                    loss = loss_fn(outputs, y)\n",
    "\n",
    "                    # the backward pass frees the graph memory, so there is no \n",
    "                    # need for torch.no_grad in this training pass\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    # scheduler.step()\n",
    "\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(x)\n",
    "                        loss = loss_fn(outputs, y.long())\n",
    "\n",
    "                # stats - whatever is the phase\n",
    "                acc = acc_fn(outputs, y)\n",
    "\n",
    "                running_acc += acc * dataloader.batch_size\n",
    "                running_loss += loss * dataloader.batch_size\n",
    "\n",
    "                if step % 100 == 0:\n",
    "                    # clear_output(wait=True)\n",
    "                    print('Current step: {}  Loss: {}  Acc: {}  AllocMem (Mb): {}'.format(step, loss, acc,\n",
    "                                                                                          torch.cuda.memory_allocated() / 1024 / 1024))\n",
    "                    # print(torch.cuda.memory_summary())\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloader.dataset)\n",
    "            epoch_acc = running_acc / len(dataloader.dataset)\n",
    "\n",
    "            print('Epoch {}/{}'.format(epoch, epochs - 1))\n",
    "            print('-' * 10)\n",
    "            print('{} Loss: {:.4f} Acc: {}'.format(phase, epoch_loss, epoch_acc))\n",
    "            print('-' * 10)\n",
    "\n",
    "            train_loss.append(epoch_loss) if phase == 'train' else valid_loss.append(epoch_loss)\n",
    "        torch.save(model.state_dict(), params_path + f'{epoch}.pth')\n",
    "    torch.save(model.state_dict(), params_path + 'final.pth')\n",
    "    time_elapsed = time.time() - start\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    return train_loss, valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "certain-badge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_metric(predb, yb):\n",
    "    return (predb.argmax(dim=1) == yb.cuda()).float().mean()\n",
    "\n",
    "def dice_metric(predb, yb):\n",
    "    dice_loss = tgm.losses.dice_loss(predb, yb)\n",
    "    return 1 - dice_loss\n",
    "\n",
    "def batch_to_img(xb, idx):\n",
    "    img = np.array(xb[idx, 0:3])\n",
    "    return img.transpose((1, 2, 0))\n",
    "\n",
    "\n",
    "def predb_to_mask(predb, idx):\n",
    "    p = torch.functional.F.softmax(predb[idx], 0)\n",
    "    return p.argmax(0).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "pretty-lithuania",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # enable if you want to see some plotting\n",
    "    visual_debug = True\n",
    "    params_path = 'models/model_epoch_'\n",
    "\n",
    "    # batch size\n",
    "    bs = 12\n",
    "\n",
    "    # epochs\n",
    "    epochs_val = 20\n",
    "\n",
    "    # learning rate\n",
    "    learn_rate = 0.01\n",
    "    \n",
    "    # datasets, 0=background+LV, 1=b+LV+M+RV, 2=??\n",
    "    datasets = ['CAMUS_resized', 'CAMUS', 'TEE']\n",
    "    curr_dataset = datasets[1]\n",
    "    outputs = 4\n",
    "\n",
    "    # sets the matplotlib display backend (most likely not needed)\n",
    "    # mp.use('TkAgg', force=True)\n",
    "    \n",
    "    # Preprocessing\n",
    "    preprocess = transforms.Compose([\n",
    "        #transforms.GaussianBlur(3, sigma=0.1)\n",
    "        #transforms.Resize((224,224))\n",
    "    ])\n",
    "\n",
    "    # load the training data\n",
    "    if curr_dataset == 'CAMUS_resized':\n",
    "        base_path = Path('/work/datasets/medical_project/CAMUS_resized')\n",
    "        data = DatasetCAMUS_r(base_path / 'train_gray', base_path / 'train_gt', transform=preprocess)\n",
    "    elif curr_dataset == 'CAMUS':\n",
    "        base_path = Path('/work/datasets/medical_project/CAMUS')\n",
    "        data = DatasetCAMUS(base_path, transform=preprocess, isotropic=False, include_es=True)\n",
    "    elif curr_dataset == 'TEE':\n",
    "        base_path = Path('/work/datasets/medical_project/TEE')\n",
    "        data = DatasetTEE()\n",
    "        \n",
    "        \n",
    "    print(len(data))\n",
    "\n",
    "    # split the training dataset and initialize the data loaders\n",
    "    train_dataset, valid_dataset, _ = torch.utils.data.random_split(\n",
    "        data,\n",
    "        (300*2, 100*2, 50*2),\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "\n",
    "    train_data = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "    valid_data = DataLoader(valid_dataset, batch_size=bs, shuffle=False)\n",
    "\n",
    "    if visual_debug:\n",
    "        fig, ax = plt.subplots(1, 2)\n",
    "        ax[0].imshow(data.open_as_array(150))\n",
    "        mask = data.open_mask(150)\n",
    "        ax[1].imshow(mask)\n",
    "        plt.show()\n",
    "        \n",
    "    xb, yb = next(iter(train_data))\n",
    "    print(xb.shape, yb.shape)\n",
    "\n",
    "    # build the Unet2D with one channel as input and 2 channels as output\n",
    "    unet = Unet2D(1, outputs)\n",
    "\n",
    "    # loss function and optimizer\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    opt = torch.optim.Adam(unet.parameters(), lr=learn_rate)\n",
    "\n",
    "    # do some training\n",
    "    train_loss, valid_loss = train(unet, train_data, valid_data, loss_fn, opt, dice_metric, epochs=epochs_val,\n",
    "                                   params_path=params_path)\n",
    "\n",
    "    # plot training and validation losses\n",
    "    if visual_debug:\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.plot(train_loss, label='Train loss')\n",
    "        plt.plot(valid_loss, label='Valid loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    # predict on the next train batch (is this fair?)\n",
    "    xb, yb = next(iter(train_data))\n",
    "    with torch.no_grad():\n",
    "        predb = unet(xb.cuda())\n",
    "\n",
    "    # show the predicted segmentations\n",
    "    if visual_debug:\n",
    "        fig, ax = plt.subplots(bs, 3, figsize=(15, bs * 5))\n",
    "        for i in range(bs):\n",
    "            ax[i, 0].imshow(batch_to_img(xb, i))\n",
    "            ax[i, 1].imshow(yb[i])\n",
    "            ax[i, 2].imshow(predb_to_mask(predb, i))\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-computer",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-capacity",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
